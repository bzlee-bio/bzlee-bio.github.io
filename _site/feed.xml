<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.9.0">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2022-04-25T14:57:23+09:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">BZLEE-Bio</title><subtitle>Nothing special</subtitle><author><name>Byungjo Lee</name></author><entry><title type="html">Algorithm study - Floyd’s cycle-finding algorithm</title><link href="http://localhost:4000/algorithm/Coding-study_floyd_algorithm/" rel="alternate" type="text/html" title="Algorithm study - Floyd’s cycle-finding algorithm" /><published>2022-04-19T00:00:00+09:00</published><updated>2022-04-19T00:00:00+09:00</updated><id>http://localhost:4000/algorithm/Coding-study_floyd_algorithm</id><content type="html" xml:base="http://localhost:4000/algorithm/Coding-study_floyd_algorithm/">&lt;h1 id=&quot;floyds-tortoise-and-hare&quot;&gt;Floyd’s Tortoise and Hare&lt;/h1&gt;
&lt;p&gt;Floyd’s Tortoise and Hare은 linked-list로 연결되어 있는 데이터 구조에서 사이클이 시작되는 부분을 효율적으로 찾게 해주는 알고리즘이다.&lt;/p&gt;

&lt;h2 id=&quot;무지해서-필요한-background&quot;&gt;무지해서 필요한 background&lt;/h2&gt;
&lt;p&gt;일단 이를 이해하기 위해서는 모듈러에 대해서 잠시 알아보자. &lt;br /&gt;
어떤 양의 정수 n과 a가 주어졌을 때, 아래와 같이 몫 q와 나머지 r을 얻을 수 있다. &lt;br /&gt;
$a = qn + r, 0 &amp;lt;= r &amp;lt; n$ &lt;br /&gt;
이것을 아래처럼 표현할 수 있다.
$a \equiv r mod n$ &lt;br /&gt;
이것을 풀어서 생각해보면, a를 n으로 나누었을 때 나머지는 r이다 라고 생각하면 될 것 같다.&lt;/p&gt;

&lt;h2 id=&quot;cycle-finding-algorithm&quot;&gt;Cycle-finding algorithm&lt;/h2&gt;
&lt;p&gt;일단 linked-list에 cycle이 있는지를 찾기 위해서 Floyd’s cycle-finding algorithm을 사용할 수 있다. &lt;br /&gt;
두개의 포인터를 두고 iteration 마다 하나는 1 step 씩, 나머지 하나는 2 step 씩 다음 list로 넘어가게 한다. 두 리스트간의 속도 차이를 이용하는 방식이다. &lt;br /&gt;
어떤 linked-list가 아래와 같은 조건으로 있다고 가정하자.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Noncycle node는 -F ~ -1 까지 존재&lt;/li&gt;
  &lt;li&gt;Cycle node는 0 ~ C-1 까지 존재 (총 개수는 C개)&lt;/li&gt;
  &lt;li&gt;Noncycle node -1 은 cycle node 0을 가리키고 있음&lt;/li&gt;
  &lt;li&gt;Cycle node 0은 cycle node C-1 가 가리키고 있음&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;포인터 1 (P1)은 매 스탭마다 1 step씩 움직이는 느린 친구, &lt;br /&gt;
포인터 2 (P2)는 매 스탭마다 2 step씩 움직이는 빠른 친구라 가정&lt;/p&gt;

&lt;p&gt;F iteration 이후에는 P1은 node 0을, P2는 cycle node 에서 어떤 node h를 가리키고 있다.
이때 P2의 현재 외치를 아래와 같이 생각할 수 있다.
$F \equiv h (mod C)$ &lt;br /&gt;
이때 h는 cycle node의 위치라고 생각할 수 있다.
P2는 실질적으로 P1에 비해서 2배 빠르게 움직이고 있고, P1은 F를 가리키고 있으니 현재 P2는 $2F$만큼 움직인 상황이다. &lt;br /&gt;
P1은 현재 cycle node 0을 가리키고 있다.&lt;/p&gt;

&lt;p&gt;이때 $C-h$만큼의 iteration을 더 수행하게 되면, P1은 C-h를 가리키게 된다. 그리고 P2도… &lt;br /&gt;
$h + 2(C-h) = 2C-h \equiv C-h (mod C)$ &lt;br /&gt;
$C-h$ 위치에 다다르게 된다. 그래서 P1 과 P2가 만나게 되어 Cycle이 있다는 것을 알 수 있게 된다. &lt;br /&gt;
(만약 cycle이 아닌 경우에는 P2가 None(Linked-list의 끝)을 만나게 될 것이다.)&lt;/p&gt;

&lt;h2 id=&quot;what-about-the-finding-the-entrance-node-of-cycle&quot;&gt;What about the finding the entrance node of cycle?&lt;/h2&gt;
&lt;p&gt;위에서는 linked-list에 cycle node 존재 여부를 확인하였다. 그렇다면 cycle entrance는 어떻게 찾을 것인가?&lt;/p&gt;

&lt;p&gt;P1은 다시 linked-list의 head node를 가리키고 P2는 앞서 찾은 P1와 P2이 처음으로 만난 C-h를 가리키게 한다. &lt;br /&gt;
여기서 다시 생각하보면, 앞서 $F \equiv h (mod C) ==&amp;gt; F = nC + h$ 로 정의가 되었으며, P2의 시작 포인트인 $C-h$로 부터 F 만큼 이동해보자. &lt;br /&gt;
$(C-h) + (nC+h) = (1+n)C$ 가 되므로 cycle node의 배수의 위치가 되므로, cycle entrance 인 node 0에 위치하게 된다. &lt;br /&gt;
이때 P1도 Noncycle node의 길이만큼인 F만큼 이동하게 되므로, 최종적으로 Cycle entrance에서 P1과 P2가 만나게 된다. &lt;br /&gt;
이때의 지점을 cycle entrance로 찾게 된다.&lt;/p&gt;

&lt;p&gt;(그림 추가 예정)&lt;/p&gt;

&lt;p&gt;약간의 정수론적인 상식을 더하여 생각해보면 재미있는 알고리즘 인 것 같으나 이런 생각을 하고 사는 사람들은 어떤 세상을 사는건지 궁금하다.&lt;/p&gt;</content><author><name>Byungjo Lee</name></author><category term="Algorithm" /><category term="Algorithm" /><category term="Floyd" /><category term="Linked-list" /><category term="Cycle-finding" /><summary type="html">Floyd’s Tortoise and Hare Floyd’s Tortoise and Hare은 linked-list로 연결되어 있는 데이터 구조에서 사이클이 시작되는 부분을 효율적으로 찾게 해주는 알고리즘이다.</summary></entry><entry><title type="html">Algorithm study - Trie</title><link href="http://localhost:4000/algorithm/Coding-study_trie/" rel="alternate" type="text/html" title="Algorithm study - Trie" /><published>2022-04-18T00:00:00+09:00</published><updated>2022-04-18T00:00:00+09:00</updated><id>http://localhost:4000/algorithm/Coding-study_trie</id><content type="html" xml:base="http://localhost:4000/algorithm/Coding-study_trie/">&lt;p&gt;모두연에서 진행하는 풀잎스쿨 중 코딩습관 기르기를 신청하게 되었다. 앞으로 알고리즘 문제를 풀다가 잘 모르는 개념을 정리해보려고 한다… &lt;br /&gt;
이번주에는 Trie라는 새로운 개념을 만나게 되어 정리하게 되었다.&lt;/p&gt;

&lt;h1 id=&quot;trie&quot;&gt;Trie&lt;/h1&gt;
&lt;p&gt;Trie는 k-ary search tree의 한 종류인 tree 데이터 구조를 말한다.&lt;/p&gt;

&lt;h2 id=&quot;무지해서-필요한-background&quot;&gt;무지해서 필요한 background&lt;/h2&gt;
&lt;p&gt;일단 &lt;em&gt;search tree&lt;/em&gt;에 대해서 간단히 알아보면, 
tree를 구성할 때 각각의 node의 key보다 작은 key를 가지는 node를 subtree의 왼쪽에, 반대로 더 큰 key를 가지는 경우 subtree의 오른쪽에 구성한다고 한다. 아마 search하는데 효율을 위해 이처럼 구성하는 것 같다. (관련 내용은 추후에 다시 정리를…) &lt;br /&gt;
그리고 &lt;em&gt;k-ary tree&lt;/em&gt;는 각각의 node가 최대 k개의 children을 가지는 것을 의미한다. 즉, binary tree는 k-ary tree 중 $k=2$인 special case라고 정의할 수 있다고 한다.&lt;/p&gt;

&lt;p&gt;즉, &lt;em&gt;k-ary search tree&lt;/em&gt;인 trie는 위에 제시된 두개의 큰 특징을 가지고 있는 tree인 것 같다.. &lt;br /&gt;
&lt;em&gt;Trie&lt;/em&gt;는 string 타입의 데이터를 이용하여 구성되는 경우가 많으며, 각 node가 string을 값으로 가지고 있는 것이 아니라 character를 가지고 있다. &lt;br /&gt;
아직 정확한 용어와 개념이 있질 않아서 더 정리를 할 수 가 없다…&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/imgs/041822_post/Trie_example.png&quot; alt=&quot;Ref; Wikipedia&quot; class=&quot;align-center&quot; /&gt;
위 이미지가 Trie의 예시이다. (참고: https://en.wikipedia.org/wiki/Trie)&lt;/p&gt;

&lt;h2 id=&quot;operations&quot;&gt;Operations&lt;/h2&gt;
&lt;p&gt;Tries를 이용하여 string key를 이용한 insertion, deletion, lookup을 수행할 수 있다.&lt;/p&gt;

&lt;p&gt;(정리중)&lt;/p&gt;</content><author><name>Byungjo Lee</name></author><category term="Algorithm" /><category term="Algorithm" /><category term="Trie" /><summary type="html">모두연에서 진행하는 풀잎스쿨 중 코딩습관 기르기를 신청하게 되었다. 앞으로 알고리즘 문제를 풀다가 잘 모르는 개념을 정리해보려고 한다… 이번주에는 Trie라는 새로운 개념을 만나게 되어 정리하게 되었다.</summary></entry><entry><title type="html">Paper review - Learning from Imbalanced Data</title><link href="http://localhost:4000/paper%20review/Paper-review/" rel="alternate" type="text/html" title="Paper review - Learning from Imbalanced Data" /><published>2022-04-05T00:00:00+09:00</published><updated>2022-04-05T00:00:00+09:00</updated><id>http://localhost:4000/paper%20review/Paper-review</id><content type="html" xml:base="http://localhost:4000/paper%20review/Paper-review/">&lt;h1 id=&quot;learning-from-imbalanced-data-2009&quot;&gt;Learning from Imbalanced Data (2009)&lt;/h1&gt;
&lt;blockquote&gt;
  &lt;p&gt;해당 포스팅의 이미지는 모두 원 논문 본문에서 발췌하였습니다.
&lt;a href=&quot;https://ieeexplore.ieee.org/document/5128907&quot;&gt;논문 링크&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;1-introduction&quot;&gt;1. Introduction&lt;/h2&gt;

&lt;p&gt;과학기술의 발전으로 인하여 데이터의 축척이 폭발적으로 증가하고 있으며 knowledge discovery, data engineering 등 다양한 분야의 발전으로 이어지고 있다. 하지만 imbalanced data에 의한 imbalanced learning problem이 있다. 이러한 imbalanced learning problem은 일반적 학습 알고리즘 성능을 저해한다는 문제가 있다. 하지만 real world에서는 데이터의 불균형은 직면할 수 밖에 없는 문제이다. &lt;br /&gt;이 논문은 아래 세 가지의 큰 틀에서 이야기를 하고 있다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;imbalance learning problem 이란&lt;/li&gt;
  &lt;li&gt;이 문제를 해결하기 위한 방법&lt;/li&gt;
  &lt;li&gt;imbalance learning 시 모델 성능 평가 방법&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;2-nature-of-the-problem&quot;&gt;2. Nature of the Problem&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;이번 섹션에서는 Imbalance data에 대한 정의와 추가적인 개념을 함께 설명하고 있다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Imbalanced data는 각 class간의 데이터 수 비율이 100:1, 1,000:1 등과 같이 하나의 class에 비해 다른 class 데이터가 매우 많은 것을 말하고 있다. 이러한 class간 불균형 (between-class imbalance)은 두 개의 class 사이 (binary), 혹은 두개보다 더 많은 class (multiclass)간에 일어날 수 있다.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;두 class 간의 불균형 예시: Mammography Dataset&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Mammography는 유방촬영술이라고 한다. 유방암을 검출하기 위한 검사하는 것을 의미하는 것 같다.&lt;/li&gt;
  &lt;li&gt;Mammography를 통해 암에 걸린 환자(positive)를 찾는 경우가 암에 걸리지 않은 환자(negative)를 찾는 경우보다 훨씬 적다. (일반검진 및 약간의 의심이 되는 경우에도 검진을 하게 될것이니.. positive case가 많을 것이라고 생각)&lt;/li&gt;
  &lt;li&gt;Mammography dataset에는 260의 “Positive” (minority class, 암환자) 샘플과 10,923의 “Negative” (majority class, 그외) 샘플이 있다.&lt;/li&gt;
  &lt;li&gt;이러한 imbalance data를 이용하여 예측 모델을 학습을 하는 경우 majority class인 negative 의 경우 분류 성능이 거의 100%에 다다르게 되며 minority class인 positive 의 경우 분류 성능이 0-10%에만 머무르게 된다.&lt;/li&gt;
  &lt;li&gt;이러한 경우 암 환자인 positive의 분류 성능이 낮은 경우는 더욱 값비싼 비용을 치루어야 하는 경우가 생길 수 있으며 이는 큰 비용을 치뤄야 하는 상황이 될 것이다. (암환자를 암이 걸렸다고 예측하지 못했을 때의 비용..) 그렇기 때문에 이러한 경우는 positive이며 minority class인 암환자를 정확하게 예측하는 것이 더욱 도움이 될 것이다.&lt;/li&gt;
  &lt;li&gt;모델 성능 평가 측면에서는 accuracy는 majority class인 negative 샘플의 예측 정확도에 크게 영향을 받기 때문에 receive operating characteristics curves (ROC curve), precision-recall curve (PR curve) 등과 같은 성능 평가 지수를 이용하는 것이 도움이 될 것이다. (Section 4)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Intrinsic / Extrinsic&lt;/strong&gt; &lt;br /&gt;
이 예제와 같이 데이터 공간 특성에 의한 불균형은 &lt;em&gt;intrinsic&lt;/em&gt; 이라고 한다. &lt;br /&gt;
그 외의 이유에 의한 불균형은 &lt;em&gt;extrinsic&lt;/em&gt; 이라고 한다. &lt;br /&gt;
(예: 데이터 공간 특성은 imbalance가 아니지만 특정 시간 간격을 두고 저장하다가 생긴 imbalance)&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Relative imbalance / Imbalance due to rare instances&lt;/strong&gt; &lt;br /&gt;
데이터의 imbalance는 &lt;em&gt;relative imbalance&lt;/em&gt;와 &lt;em&gt;imbalance due to rare instances&lt;/em&gt; 두가지 경우도 있다. 예를 들어 앞선 mammography dataset에서 두 클래스간의 데이터 차이가 100:1 인 경우,&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;총 데이터 수가 200,000인 경우 경우 minority class의 데이터가 2,000개로 majority class에 비해 relative imbalance 하나 절대적인 수는 rare 하지 않다.&lt;/li&gt;
  &lt;li&gt;Imbalance due to rare instances는 minority class의 데이터의 절대적 양이 극도로 적은 경우를 의미한다.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;Within-class imbalance&lt;/strong&gt; &lt;br /&gt;
동일 클래스 내에서 일부 subconcept과 나머지 concept간의 데이터 imbalance를 의미한다. Concept은 2개의 타겟($Target1{\cup}Target2$)을 동일한 class로 두는 것이며, subconcept은 각각의 타겟($Target1$, $Target2$)를 의미하며 이는 concept의 subconcept(subconcepts of concept) 이라고 할 수 있다. (참고: &lt;a href=&quot;https://link.springer.com/article/10.1007/s00500-020-04828-5&quot;&gt;논문&lt;/a&gt;)&lt;/p&gt;

&lt;p&gt;지금까지 나온 개념은 아래의 그림으로 확인할 수 있다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/imgs/040622_post/fig2.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;
(a)의 경우 between-class imbalance의 예로 원형 데이터가 majority class, 별모양 데이터가 minority class이며 relative imbalance한 것을 알 수 있다. 여기서 minority class의 경우 noise sample이 있는 것을 알 수 있다. Majority class와 minority class 사이에 겹치는 구간이 없는 것을 확인할 수 있다. &lt;br /&gt;
(b)의 경우 A와 B인 majority class와 minority class 사이에 데이터가 겹쳐있는 것을 볼 수 있다. 이는 높은 data complexity를 가진다고 할 수 있다. &lt;br /&gt;
A-D, B-C는 concept과 subconcept간의 관계로 이루어져 있으며 각각의 경우 모두 subconcept이 concept에 비해 imbalance한 데이터를 가지고 있으며 within-class imbalance라고 할 수 있다. &lt;br /&gt;
C의 경우에는 minority class의 subconcept임과 동시에 데이터 수가 매우 적은것을 알 수 있다. 이는 Imbalance due to rare instances의 예라고 볼 수 있다.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Within-class imbalance&lt;/strong&gt;로 인한 모델 성능 저하는 다음과 같이 일어날 수 있다. 분류 모델은 여러 small disjunct (*A small disjunct is a disjunct that covers only a few training examples) 을 아우르며 concept을 분류할 수 있는 규칙을 찾으려 할 것이다. 이때, homogeneous concept인 경우 전반적으로 분류할 수 있는 규칙을 찾아낼 수 있지만 heterogeneous concept의 경우에는 subconcept의 특징을 충분히 찾아내지 못할 것이다.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Noise sample&lt;/strong&gt;에 의한 문제도 발생할 수 있다. 충분한 데이터를 가지고 있는 majority class에서는 일부 noise sample이 있더라도 noise가 있지 않은 충분한 양의 데이터가 있기 때문에 이러한 문제가 거의 일어나지 않으나, 데이터가 적은 minority class에서는 몇몇의 noise sample에 의해 분류 모델에게 크게 혼동을 주어 class를 분류하기 위한 규칙을 찾는데 있어서 문제가 될 수 있다.&lt;/p&gt;

&lt;h2 id=&quot;3-the-state-of-the-art-solutions-for-imbalanced-learning&quot;&gt;3. The State-of-the-art Solutions for Imbalanced Learning&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;이번 섹션에서는 Imbalanced learning에 의한 문제를 극복하기 위한 방법들을 소개하고 있다.&lt;/li&gt;
  &lt;li&gt;논문 제목에 sota solution이라는 제목을 사용하였지만, 논문이 발간된 지는 너무 오래되었기 때문에 그 당시의 sota 모델 예시라고 볼 수 있다.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;31-sampling-methods-for-imbalanced-learning&quot;&gt;3.1 Sampling Methods for Imbalanced Learning&lt;/h3&gt;
&lt;p&gt;데이터 샘플링을 통하여 데이터 분포를 균일하게 맞추어 주는 방식을 말한다. 많은 경우에 샘플링을 통해 데이터 수의 균형을 맞춘 결과 더 좋은 모델 성능을 보였다고 이야기하고 있다.&lt;/p&gt;

&lt;h4 id=&quot;311-random-oversampling-and-undersampling&quot;&gt;3.1.1 Random Oversampling and Undersampling&lt;/h4&gt;
&lt;p&gt;&lt;em&gt;Random oversampling&lt;/em&gt;이란 minority class의 데이터셋 $S_{min}$으로 부터 랜덤하게 데이터를 샘플링하여 $S_{min}$에 데이터를 추가하게 된다. 즉, 샘플되어 추가된 데이터는 중복되어 있는 상태이다. 이러한 방식은 중복된 minority class 데이터로 인하여 분류 모델 학습 시 overfitting이 일어날 수 있다.
&lt;em&gt;Random unversampling&lt;/em&gt;이란 majority class의 데이터셋 $S_{maj}$으로 부터 랜덤하게 데이터를 제거하는 방식을 말한다. 이러한 방식은 제거된 데이터로부터 얻을 수 있는 중요한 특징을 분류 모델이 얻게되지 못할 수 있다는 문제가 있다.&lt;/p&gt;

&lt;h4 id=&quot;312-informed-undersampling&quot;&gt;3.1.2 Informed Undersampling&lt;/h4&gt;
&lt;p&gt;여기서는 &lt;em&gt;EasyEnsemble&lt;/em&gt;, &lt;em&gt;BalanceCascade&lt;/em&gt;, &lt;em&gt;undersampling using kNN&lt;/em&gt; 세가지 방법을 소개한다. &lt;br /&gt;
&lt;em&gt;&lt;b&gt;EasyEnsemble&lt;/b&gt;&lt;/em&gt;은 majority class의 데이터를 undersampling 하게 되는데 이때 하나가 아닌 여러개의 undersampled majority class 데이터셋을 만들게 된다. 각각의 undersampled majority class 데이터셋과 하나의 minority class 샘플을 이용하여 여러개의 분류 모델을 이용하여 학습하게 된다. 여기서 undersampled majority class 데이터의 경우 여러개의 서브셋으로 구성되어 있고, 하나의 분류모델을 학습 시 서브셋 중 하나를 활용하게 된다. 이렇게 되면 여러개의 모델은 각각의 undersampled majority class를 통해 구성된 서브셋과 minority class 데이터셋을 모델 학습에 이용하게 되기 떄문에 각기 다른 데이터셋을 이용하여 학습하게 된다. 최종 예측 결과는 여러개의 분류 모델의 예측 결과를 종합하는 앙상블 방식을 이용하게 된다.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;&lt;b&gt;BalanceCascade&lt;/b&gt;&lt;/em&gt;란 majority class $S_{maj}$로부터 undersample 된 데이터셋 $E$와 minority class의 데이터셋 $S_{min}$을 이용하여 분류 모델을 학습하게 된다. 학습된 모델을 이용하여 $E$를 분류하고 모델이 제대로 분류한 샘플을 $N_{maj}^{\ast}$ 라고 한다면 해당 데이터를 표현해 줄 수 있는 데이터는 중복되어 있다고 생각하여 $S_{maj}$로부터 제거하게 된다. 이와 같은 작업을 반복적으로 수행하게 되어 데이터를 스크리닝(?) 하게 된다.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;&lt;b&gt;Undersampling using K-nearest neighbor (KNN)&lt;/b&gt;&lt;/em&gt;은 KNN을 이용하여 undersampling 하는 기법이다. 여기에는 NearMiss-1, NearMiss-2, NearMiss-3 등이 있다. NearMiss-1 의 경우 각각의 majority 샘플에 대해 &lt;b&gt;가장 가까운&lt;/b&gt; 3개의 minority sample 까지의 거리의 평균이 작은 순으로 선택한다. NearMiss-2의 경우 각각의 majority 샘플에 대해 &lt;b&gt;가장 먼&lt;/b&gt; 3개의 minority sample 까지의 거리의 평균이 작은 순으로 선택한다. NearMiss-3의 경우 각 minority 샘플이 가장 가까운 majority sample을 선택한다.&lt;/p&gt;

&lt;h4 id=&quot;313-synthetic-sampling-with-data-generation-smote&quot;&gt;3.1.3 Synthetic Sampling with Data Generation (SMOTE)&lt;/h4&gt;
&lt;p&gt;&lt;em&gt;SMOTE&lt;/em&gt;란 minority class의 새로운 데이터를 존재하는 데이터를 기반으로 만들어내는 방식이다. Minority class의 샘플 중 하나를 선택하고 같은 class 의 샘플 중 가장 가까운 K개의 샘플 중 1개를 선택하게 된다. 이 두 샘플 사이에 새로운 데이터를 추가하는 방식이다. 데이터 추가는 아래와 같이 된다.&lt;/p&gt;
&lt;center&gt;$x_{new}=x_{i}+({\hat{x}}_{i}-x_i)\times\delta$&lt;/center&gt;
&lt;p&gt;$x_{new}$: 새로 추가된 데이터, $x_i$: minority class 샘플 데이터, $\hat{x}_{i}$: $x_i$ 샘플에서 가장 가까운 k개의 minority 샘플 중 1개, $\delta$: [0, 1] 중 랜덤 수&lt;/p&gt;

&lt;p&gt;SMOTE를 그림으로 살펴보자.
&lt;img src=&quot;/imgs/040622_post/fig3.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;위 그림에서 (a)의 경우 minority class(별모양)의 샘플 $x_i$와 그 주위에 가장 가까운 6개의 샘플 이 선으로 연결되어 있는 것을 볼 수 있다. (b)의 경우 6개 중 아래에 있는 $\hat{x}_i$와 $x_i$ 사이에 데이터를 생성하는 예시를 보여주고 있다.&lt;/p&gt;

&lt;h4 id=&quot;314-adaptive-synthetic-sampling&quot;&gt;3.1.4 Adaptive Synthetic Sampling&lt;/h4&gt;
&lt;p&gt;앞서 이야기한 SMOTE 알고리즘의 경우 minority 와 majority class간의 경계를 고려하지 않고 데이터를 생성하기 때문에 두 class간 경계를 설정하는 문제에 있어서는 한계가 있을 수 있다고 한다. 이러한 문제에 대응하기 위해 &lt;em&gt;Borderline-SMOTE&lt;/em&gt;와 &lt;em&gt;Adaptive Synthetic Sampling (ADA-SYN)&lt;/em&gt;이 제시되었다.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;Borderline-SMOTE&lt;/em&gt;&lt;/strong&gt;
각각의 minority class 샘플로부터 가장 가까운 근처 (앞서부터 계속 nearest neighbors의 개념을 사용중) 샘플 m개 중 majority class 샘플의 개수 $N_{k-maj}$를 계산한다. 계산된 값을 이용하여 아래와 같이 경우를 나누어 알고리즘을 수행한다.&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;$m/2 \leq N_{k-maj} &amp;lt; m$ 인 경우, “DANGER” 로 분류한다. DANGER로 분류된 샘플들을 이용하여 SMOTE를 수행하게 된다.&lt;/li&gt;
  &lt;li&gt;$N_{k-maj} = m$ 인 경우, “NOISE” 로 분류한다. 이러한 경우는 SMOTE를 수행하지 않는다.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;아래 그림을 보면 더 확실하게 이해할 수 있다.
&lt;img src=&quot;/imgs/040622_post/fig4.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Minority class의 샘플 중 majority class 샘플과 인접한, 즉 경계선에 있는 샘플들에 대해서 생성을 진행하는 것을 알 수 있다.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;ADASYN&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;생성할 샘플의 수를 다음과 같이 결정하게 된다. Majority class의 샘플 수 와 minority class의 샘플 수 차이에 $\beta$를 곱한 수를 선택하게 된다.
$G=(|S_{maj}|-|S_{min}|)\times\beta$ &lt;br /&gt;
$|S_{maj}|$: Majority class의 샘플 수, $|S_{min}|$: Minority class의 샘플 수, $\beta\in[0,1]$&lt;/li&gt;
  &lt;li&gt;Minority class 각각의 샘플 $x_i \in S_{min}$에 대해서 K개의 가까운 샘플을 찾고 아래와 같이 $\Gamma_i$ 값을 구한다. 
$\Gamma_i=\frac{\Delta_i/K}{Z}, i=1,…,|S_{min}|$ &lt;br /&gt;
$\Delta_i: x_i$로부터 가장 가까운 k개의 샘플 중 majority class 샘플의 수, $Z:$ Normalization 상수, $\sum\Gamma_i=1$이 되도록 설정 &lt;br /&gt;
$\Gamma_i$는 각 샘플당 생성되어야 할 생성 샘플 비율이라고 보면 된다.&lt;/li&gt;
  &lt;li&gt;앞서 각 샘플당 생성되어야 할 생성 샘플 비율을 설정하였으니 각 샘플 당 생성되어야 할 샘플 수($g_i$)를 아래와 같이 계산하게 된다. &lt;br /&gt;
$g_i=\Gamma_i\times G$ &lt;br /&gt;
이 후에 생성은 논문에는 설명되어 있지 않으나 아마 앞서 소개된 SMOTE를 사용할 수 있지 않을까 싶다.&lt;/li&gt;
&lt;/ol&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;em&gt;Borderline-SMOTE&lt;/em&gt;의 경우 특정 조건에 맞는 샘플에 대해서 균일하게 데이터를 생성하게 하였다면 (주로 major, minority class가 인접해 있는 경계 부분), ADASYN은 경계 부분에서도 majority class와 더욱 더 인접해 있는 샘플에 가중치를 주어 더 많은 데이터를 생성하게 된다.&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;315-sampling-with-data-cleaning-techniques&quot;&gt;3.1.5 Sampling with Data Cleaning Techniques&lt;/h4&gt;
&lt;p&gt;이번에는 앞서 소개된 샘플링 기법을 통해 얻어진 데이터에 대해서 중복된 뎉이터를 제거하는 기법인 Tomek links 에 대해서 소개하고 있다. &lt;br /&gt;
Tomek links란 가장 가까우면서 각각의 샘플이 다른 클래스에 속해 있는 샘플 쌍을 의미한다. 이러한 샘플 쌍 $(x_i, x_j)$은 다음과 같이 정의가 된다.
$(x_i, x_j), x_i \in S_{min}, x_j \in S_{maj}$
두 샘플 간 거리는 $d(x_i, x_j)$ 로 한다.&lt;/p&gt;

&lt;p&gt;여기서 $(x_i, x_j)$는 아래의 조건을 만족할 때 Tomek link라 한다.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;$d(x_i, x_k) &amp;lt; d(x_i, x_j)$ 또는 $d(x_k, x_j) &amp;lt; d(x_i, x_j)$를 만족하는 $x_k$가 존재하지 않는 경우
이와같이 정의가 된 Tomek link는 둘 중 하나의 요소가 노이즈 이거나 두 요소가 경계에 있는 요소라고 생각할 수 있다. 앞서 제시된 데이터 생성 후 데이터 클리닝을 위해 Tomek links를 “cleanup” 하는 방법을 취할 수 있다. 이러한 예제는 그림으로 보면 이해가 더 쉽다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/imgs/040622_post/fig5.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;
위 그림에서 (a)는 원 데이터 분포를 보여주고 있다. (b)는 SMOTE를 통해 minority class인 별 모양 데이터가 추가된 것을 볼 수 있다. 이때 majority class와 minority class 사이에 혼잡하게 섞여있는 형태를 볼 수 있다. (c) Tomek links로 확인된 데이터를 볼 수 있다. (d) Tomek links로 확인덴 데이터를 “cleanup” 한 후의 결과이다.&lt;/p&gt;

&lt;p&gt;위처럼 SMOTE를 진행한 후 중복(overlap) 되는 데이터가 많아진 것을 확인할 수 있다. Tomek links를 확인하여 데이터를 더 깔끔하게 만들 수 있는 것을 볼 수있다. (개인적인 생각으로는 애초에 원 데이터셋이 너무 경계가 뚜렷하지 않은 좋지 않은 예이지 않나 싶긴 하다..)&lt;/p&gt;

&lt;h4 id=&quot;316-cluster-based-sampling-method&quot;&gt;3.1.6 Cluster-Based Sampling Method&lt;/h4&gt;
&lt;p&gt;클러스터링 기반 샘플링 방법은 이전 방법들과는 다르게 특정 타겟에 맞추어 샘플링을 할 수 있는 기법이다. 여기서는 cluster-based oversampling (CBO) algorithm 이 소개되고 있다. &lt;br /&gt;
CBO 알고리즘은 K-means clustering 기법을 기반으로 한다. &lt;br /&gt;
각각의 class에서 K 개의 샘플을 랜덤하게 샘플링한 후 샘플된 데이터의 feature에 대해서 평균 값을 구하여 mean feature vector를 구하게 된다. 이러한 mean feature vector는 샘플링 된 데이터 클러스터의 중앙을 가리키는 벡터가 될 것이다. &lt;br /&gt;
선택되지 않은 샘플 중 1개에 대해서 각 cluster의 mean feature vector와의 euclidian distance를 구하게 된다. 각 class 중 distance가 가장 가까운 cluster 로 샘플을 추가하게 되고 해당 cluster의 mean feature vector를 다시 계산하게 된다. &lt;br /&gt;
이러한 방식으로 선택되지 않은 샘플 모두에 대해서 진행하게 된다. 이때 샘플이 가장 많은 majority class와 샘플 수가 동일하게 되도록 minority class에 대해서 oversampling 을 진행하게 된다. 아래 그림으로 내용을 확인해보자.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/imgs/040622_post/fig6.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;
(a)는 원 데이터의 분포이다. (b)는 각 class에서 K=3 개의 샘플을 샘플링한 후 mean feature vector가 가리키는 위치 (삼각형, clustering mean)을 보여주고 있으며 샘플된 포인트와 각 cluster mean의 distance를 보여주고 있다. (c)는 새로 계산되는 cluster mean을 보여주고 있다. (d)는 minority class 인 D, E에 대해 cluster-based oversampling 을 진행한 후의 모습이다. 여기서 oversampling을 하는 경우 앞서 소개된 데이터 생성 예제와 같은 방법을 사용하여 진행한다. 이러한 방법은 within-class imbalance와 between-class imbalance에 효과적이라고 한다.&lt;/p&gt;

&lt;p&gt;이 방법 같은 경우는 개인적으로는 효과적인 것인가에 대한 의문점이 든다..&lt;/p&gt;

&lt;h4 id=&quot;317-integration-of-sampling-and-boosting&quot;&gt;3.1.7 Integration of Sampling and Boosting&lt;/h4&gt;
&lt;p&gt;앞서 소개된 샘플링 기반 방법과 boosting 기반의 방법을 사용하는 다양한 연구사례가 있음. 예를들어 SMOTEBoost는 SMOTE와 Adaboost를 함께 사용하는 방법이다. 여기서 매번 boosting 시 SMOTE 방법을 통하여 새로운 데이터를 생성해 낸다. 즉, 매 boosting을 통해 학습된 분류 모델은 각기 다른 데이터를 기반으로 하고 있기 때문에 ensemble 을 통하여 더 정확한 결과를 얻을 수 있다.&lt;/p&gt;

&lt;h3 id=&quot;32-cost-sensitive-methods-for-imbalanced-learning&quot;&gt;3.2 Cost-Sensitive Methods for Imbalanced Learning&lt;/h3&gt;
&lt;p&gt;앞서 소개된 샘플링 방법은 클래스 간 데이터 분포를 균일하게 맞추어주는 방향으로 진행된다. 지금부터 소개 될 cost-sensitive learning은 특정 데이터의 오분류에 대한 내용을 반영한 cost 함수를 사용하는 방법이다.&lt;/p&gt;

&lt;h4 id=&quot;321-cost-sensitive-learning-framework&quot;&gt;3.2.1 Cost-Sensitive Learning Framework&lt;/h4&gt;
&lt;p&gt;Cost-sensitive learning 방법을 이해하기 위해서 필요한 cost matrix에 대해서 알아보자. &lt;br /&gt;
Cost matrix는 예측 결과에서 잘못된 분류에 의한 페널티를 수치적으로 표현한 것이라고 생각할 수 있다. Cost matrix의 요소는 아래와 같이 정의할 수 있다.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;$C(Min, Maj)$: Majority class를 minority class로 잘못 분류한 것에 대한 cost,&lt;/li&gt;
  &lt;li&gt;$C(Min, Maj)$: Minority class를 majority class로 잘못 분류한 것에 대한 cost.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;각 class에 대한 cost를 정의할 수 있다면, 모델 학습 시 모든 cost를 감소시키는 방향으로 학습을 진행하게 되는 것이 cost-sensitive learning 이다. (앞서 나온 kNN 등의 방법과는 다르게 cost 기반 학습방법이 해당되는 것 같다.)&lt;/p&gt;

&lt;p&gt;앞서 나온 개념을 binary class가 아닌 multiclass 개념으로 확장시키면 아래와 같은 테이블을 얻을 수 있다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/imgs/040622_post/fig7.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt; &lt;br /&gt;
여기서 우리는 입력된 데이터 $\textbf{x}$에 대해 모델로 부터 예측된 predicted class $i$ 에 대한 conditional risk를 다음과 같이 정의할 수 있다. &lt;br /&gt;
$R(i|\textbf{x})=\sum_{j}P(j|\textbf{x})C(i,j), P(j|\textbf{x}):$ 입력 데이터 $\textbf{x}$ 중 Class $j$의 비율&lt;/p&gt;

&lt;p&gt;지금까지는 cost matrix에 대해서 알아보았고, 지금부터 cost-sensitive learning에 대해서 알아보자. 크게 3가지로 나눌 수 있다.&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;데이터공간 상에서 가장 넓은 학습 분포를 가지도록 하는 cost-sensitive bootstrap sampling 접근방법&lt;/li&gt;
  &lt;li&gt;앙상블 기법을 활용한 cost-minimizing 테크닉&lt;/li&gt;
  &lt;li&gt;Cost-sensitive function을 직접 모델 학습에 적용하는 경우&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;위 세가지 기법에 대해서 방법론을 지금부터 살펴보자.&lt;/p&gt;

&lt;h4 id=&quot;322-cost-sensitive-dataspace-weightning-with-adaptive-boosting&quot;&gt;3.2.2 Cost-Sensitive Dataspace Weightning with Adaptive Boosting&lt;/h4&gt;
&lt;p&gt;(작성중…)&lt;/p&gt;</content><author><name>Byungjo Lee</name></author><category term="Paper review" /><category term="Data Imbalance" /><category term="Imbalance Learning" /><summary type="html">Learning from Imbalanced Data (2009) 해당 포스팅의 이미지는 모두 원 논문 본문에서 발췌하였습니다. 논문 링크</summary></entry></feed>